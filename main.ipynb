{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from policy_net import Policy_net\n",
    "from ppo import PPOTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "EPISODES = int(1e5)\n",
    "GAMMA = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env         = gym.make('CartPole-v1')       # Instancia o ambiente CartPole\n",
    "    env.seed(0)                                 #\n",
    "    ob_space    = env.observation_space         # Descrevem o formato de observações válidas do espaço\n",
    "    Policy      = Policy_net('policy', env)     # Cria a rede de Politica\n",
    "    Old_Policy  = Policy_net('old_policy', env) # Cria a rede de politica antiga\n",
    "    PPO         = PPOTrain(Policy, Old_Policy, gamma=GAMMA)\n",
    "    saver       = tf.train.Saver()              #\n",
    "\n",
    "    with tf.Session() as sess:  # Bloco da sessão \n",
    "        writer = tf.summary.FileWriter('./log/train', sess.graph)   # Define diretório de logs\n",
    "        sess.run(tf.global_variables_initializer())                 # Inicializa as redes\n",
    "\n",
    "        obs = env.reset()   # Reseta o ambiente e obtêm a primeira observação\n",
    "        reward = 0          # Armazena as recompensas\n",
    "        success_num = 0     # Contador de sucessos\n",
    "\n",
    "        for episode in range(EPISODES): # Loop do episodio\n",
    "            observations = []           # Array pra armazenar as observações\n",
    "            actions = []                # Array pra armazenar as ações\n",
    "            v_preds = []                # Array pra armazenar as previsões\n",
    "            rewards = []                # Array pra armazenar as recompensas\n",
    "            run_policy_steps = 0        # Contador de passos em cada episodio\n",
    "            env.render()                # Renderiza o ambiente\n",
    "\n",
    "            while True: # Run policy RUN_POLICY_STEPS which is much less than episode length\n",
    "                        # Execute a política RUN_POLICY_STEPS, que é muito menor que a duração do episódio\n",
    "                run_policy_steps += 1                               # Incrementa contador de passos de cada episodio\n",
    "                obs = np.stack([obs]).astype(dtype=np.float32)      # prepare to feed placeholder Policy.obs\n",
    "                act, v_pred = Policy.act(obs=obs, stochastic=True)  # Corre a rede neural e obtêm uma ação e o V previsto\n",
    "\n",
    "                act     = act.item()        # Transforma um array do numpy \n",
    "                v_pred  = v_pred.item()     # em um objeto scalar do Python\n",
    "\n",
    "                observations.append(obs)    # Adiciona a observação ao buffer de observações\n",
    "                actions.append(act)         # Adiciona a ação ao buffer de ações\n",
    "                v_preds.append(v_pred)      # Adiciona a v_pred ao buffer de v_pred\n",
    "                rewards.append(reward)      # Adiciona a recompensa ao buffer de recompensa\n",
    "\n",
    "                next_obs, reward, done, info = env.step(act)    # envia a ação ao ambiente e recebe a próxima observação, a recompensa e se o passo terminou\n",
    "\n",
    "                if done:                # Se o done for verdadeiro ...\n",
    "\n",
    "                    v_preds_next = v_preds[1:] + [0]    # [1:] seleciona do segundo elemento da lista em diante e + [0] adiciona um elemento de valor zero no final da lista\n",
    "                                                        # next state of terminate state has 0 state value\n",
    "                                                        # próximo estado do estado final tem 0 valor de estado\n",
    "                    obs = env.reset()   #   Redefine o ambiente\n",
    "                    reward = -1         #   define a recompensa como -1 (?)\n",
    "                    break               #   Sai do loop while\n",
    "                else:                   # Senão...\n",
    "                    obs = next_obs      #   Armazena em obs a próxima observação\n",
    "\n",
    "            # Armazena em log para visualização no tensorboard\n",
    "            writer.add_summary(tf.Summary(value=[tf.Summary.Value(tag='episode_length', simple_value=run_policy_steps)]), episode)\n",
    "            writer.add_summary(tf.Summary(value=[tf.Summary.Value(tag='episode_reward', simple_value=sum(rewards))]),     episode)\n",
    "\n",
    "            # Condicional para finalizar o teste\n",
    "            if sum(rewards) >= 195:                         # Se a soma das recompensas for maior ou igual 195\n",
    "                success_num += 1                            #   Incrementa o contador de sucessos\n",
    "                if success_num >= 100:                      #   Se ocorrerem 100 sucessos\n",
    "                    saver.save(sess, './model/model.ckpt')  #       Salva a sessão\n",
    "                    print('Clear!! Model saved.')           #       Escreve na tela\n",
    "                    break                                   #       Sai do loop\n",
    "            else:                                           # senão, \n",
    "                success_num = 0                             #   zera o contador de sucessos\n",
    "            \n",
    "            print(\"EP: \",episode,\" Rw: \",sum(rewards))      # Escreve na tela o numero do episodio e a recompensa\n",
    "\n",
    "            gaes = PPO.get_gaes(rewards=rewards, v_preds=v_preds, v_preds_next=v_preds_next) # ?\n",
    "\n",
    "            # Converte lista em NPArray para alimentar o tf.placeholder\n",
    "            newshape=[-1] + list(ob_space.shape) # cria um array [-1, 4]\n",
    "            observations = np.reshape(observations, newshape=newshape) # antes, cada linha de observations era um array idependente. depois do reshape, observations passou ser um array só com varias linhas.\n",
    "            \n",
    "            actions      = np.array(actions).astype(dtype=np.int32)\n",
    "            \n",
    "            rewards      = np.array(rewards).astype(dtype=np.float32)\n",
    "            v_preds_next = np.array(v_preds_next).astype(dtype=np.float32)\n",
    "            gaes         = np.array(gaes).astype(dtype=np.float32)\n",
    "            gaes         = (gaes - gaes.mean()) / gaes.std() # subtrai dos itens de gaes a media de todos os itens de gaes e divide todos pelo desvio padrao de gaes\n",
    "\n",
    "            PPO.assign_policy_parameters()\n",
    "\n",
    "            inp = [observations, actions, rewards, v_preds_next, gaes]  # Cria um array com 5 colunas: observações, ações, recompensas, \n",
    "\n",
    "            # Treina\n",
    "            for epoch in range(4):\n",
    "                sample_indices  = np.random.randint(low=0, high=observations.shape[0], size=64) # índices estão em [baixo, alto]\n",
    "                sampled_inp=[]\n",
    "                for a in inp:\n",
    "                    sampled_inp.append(np.take(a=a, indices=sample_indices, axis=0))    # amostra de dados de treinamento\n",
    "                PPO.train(obs=sampled_inp[0], actions=sampled_inp[1], rewards=sampled_inp[2], v_preds_next=sampled_inp[3], gaes=sampled_inp[4])\n",
    "\n",
    "            summary = PPO.get_summary(obs=inp[0],actions=inp[1],rewards=inp[2],v_preds_next=inp[3],gaes=inp[4])[0]\n",
    "\n",
    "            writer.add_summary(summary, episode)\n",
    "        writer.close()  # Final do episódio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from policy_net import Policy_net\n",
    "from ppo import PPOTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATION = int(3 * 10e5)\n",
    "GAMMA = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    env.seed(0)\n",
    "    ob_space = env.observation_space\n",
    "    Policy = Policy_net('policy', env)\n",
    "    Old_Policy = Policy_net('old_policy', env)\n",
    "    PPO = PPOTrain(Policy, Old_Policy, gamma=GAMMA)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter('./log/test', sess.graph)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, 'model/model.ckpt')\n",
    "        obs = env.reset()\n",
    "        reward = 0\n",
    "        success_num = 0\n",
    "\n",
    "        for iteration in range(ITERATION):  # episode\n",
    "            observations = []\n",
    "            actions = []\n",
    "            v_preds = []\n",
    "            rewards = []\n",
    "            run_policy_steps = 0\n",
    "            env.render()\n",
    "            while True:  # run policy RUN_POLICY_STEPS which is much less than episode length\n",
    "                run_policy_steps += 1\n",
    "                obs = np.stack([obs]).astype(dtype=np.float32)  # prepare to feed placeholder Policy.obs\n",
    "                act, v_pred = Policy.act(obs=obs, stochastic=False)\n",
    "\n",
    "                act     = act.item()\n",
    "                v_pred  = v_pred.item()\n",
    "\n",
    "                observations.append(obs)\n",
    "                actions.append(act)\n",
    "                v_preds.append(v_pred)\n",
    "                rewards.append(reward)\n",
    "\n",
    "                next_obs, reward, done, info = env.step(act)\n",
    "\n",
    "                if done:\n",
    "                    v_preds_next = v_preds[1:] + [0]  # next state of terminate state has 0 state value\n",
    "                    obs = env.reset()\n",
    "                    reward = -1\n",
    "                    break\n",
    "                else:\n",
    "                    obs = next_obs\n",
    "\n",
    "            writer.add_summary(tf.Summary(value=[tf.Summary.Value(tag='episode_length', simple_value=run_policy_steps)])\n",
    "                               , iteration)\n",
    "            writer.add_summary(tf.Summary(value=[tf.Summary.Value(tag='episode_reward', simple_value=sum(rewards))])\n",
    "                               , iteration)\n",
    "\n",
    "            # end condition of test\n",
    "            if sum(rewards) >= 195:\n",
    "                success_num += 1\n",
    "                if success_num >= 100:\n",
    "                    print('Iteration: ', iteration)\n",
    "                    print('Clear!!')\n",
    "                    break\n",
    "            else:\n",
    "                success_num = 0\n",
    "\n",
    "            gaes = PPO.get_gaes(rewards=rewards, v_preds=v_preds, v_preds_next=v_preds_next)\n",
    "\n",
    "            # convert list to numpy array for feeding tf.placeholder\n",
    "            observations = np.reshape(observations, newshape=[-1] + list(ob_space.shape))\n",
    "            actions = np.array(actions).astype(dtype=np.int32)\n",
    "            rewards = np.array(rewards).astype(dtype=np.float32)\n",
    "            v_preds_next = np.array(v_preds_next).astype(dtype=np.float32)\n",
    "            gaes = np.array(gaes).astype(dtype=np.float32)\n",
    "            gaes = (gaes - gaes.mean()) / gaes.std()\n",
    "\n",
    "            inp = [observations, actions, rewards, v_preds_next, gaes]\n",
    "\n",
    "            summary = PPO.get_summary(obs=inp[0],\n",
    "                                      actions=inp[1],\n",
    "                                      rewards=inp[2],\n",
    "                                      v_preds_next=inp[3],\n",
    "                                      gaes=inp[4])[0]\n",
    "\n",
    "            writer.add_summary(summary, iteration)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
